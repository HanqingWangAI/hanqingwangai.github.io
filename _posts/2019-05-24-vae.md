---
layout: post
title: Tutorial on VAE
date: 2019-02-14
author: Hanqing Wang
cover: '/assets/files/posts/3d_recon_19/network.png'
thumbnail: '/assets/files/posts/3d_recon_19/result.png'
authors: <strong>Hanqing Wang</strong>, Jiaolong Yang, Wei Liang and Xin Tong
# venue: Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), 2019, <strong>(Oral)</strong>
# paper: https://arxiv.org/pdf/1809.03451.pdf
# code: https://github.com/qweas120/PSVH-3d-reconstruction
# demo: http://iitlab.bit.edu.cn/mcislab/~liangwei/projects/aaai19_3d_recon/assets/supplementary_2.mp4
# slide: '/assets/files/posts/3d_recon_19/Deep Single-View 3D Object Reconstruction with Visual Hull.pptx'
tags: "3D&nbspReconstruction Computer&nbspVision Deep&nbspLearning"
---

# purpose
unsupervised learning of complicated distributions.

generative model: One straightforward kind of generative model simply allows us to compute P(X) numerically.

VAEs have already shown promise in generating many kinds of complicated data, including handwritten digits, faces, house numbers, CIFAR images, physical models of scenes, segmentation, and predicting the future from static images.


### Characteristics of VAE
- Weak assumption
- Fast training

### Latent Variable Models
- Take hand written document as an example, Intuitively, it helps if the model first decides which character to generate before it assigns a value to any specific pixel. This kind of decision is formally called a latent variable, annotated as $z$. $z$ is called latent since given just a character produced by the model, we don't necessarily know which settings of the latent variables generated the character.

